{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport time\n\nimport os\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport torchvision\n\n\nNUM_CLASSES = 8\nIGNORE_INDEX = 0\nIMG_SIZE = 512\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nAMP_ENABLED = (DEVICE == \"cuda\")\nfrom matplotlib.patches import Patch\n\nCLASS_INFO = {\n    1: (\"Background\",    (128,   0,   0)),\n    2: (\"Building\",       (  0, 128,   0)),\n    3: (\"Road\",          (128, 128,   0)),\n    4: (\"Water\",          (  0,   0, 128)),\n    5: (\"Barren\",        (128,   0, 128)),\n    6: (\"Forest\",       (  0, 128, 128)),\n    7: (\"Agriculture\",    (128, 128, 128)),\n}\n\n\nCOLOR_MAP = np.array([\n    [0, 0, 0],         # 0: ignored\n    [128, 0, 0],        # 1: background (incl. playground)\n    [0, 128, 0],       # 2: building\n    [128, 128, 0],    # 3: road\n    [0, 0, 128],       # 4: water\n    [128, 0, 128],    # 5: barren\n    [0, 128, 128],      # 6: forest\n    [128, 128, 128],   # 7: agriculture\n], dtype=np.uint8)\n\n\nCLASS_NAMES = [\n    \"No-data (ignored)\",        # 0\n    \"Background\",                # 1 (includes playground)\n    \"Building\",                  # 2\n    \"Road\",                      # 3\n    \"Water\",                      # 4\n    \"Barren\",                   # 5\n    \"Forest\",                     # 6\n    \"Agriculture\",              # 7\n]\n\n\nclass ConvBNReLU(nn.Module):\n    def __init__(self, in_ch, out_ch, k=3, s=1, p=1, groups=1, use_dropout=False, p_drop=0.1):\n        super().__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, k, s, p, bias=False, groups=groups)\n        self.bn = nn.BatchNorm2d(out_ch)\n        self.relu = nn.ReLU(inplace=True)\n        self.drop = nn.Dropout2d(p_drop) if use_dropout else nn.Identity()\n        nn.init.kaiming_normal_(self.conv.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.drop(x)\n        return x\n\nclass DepthwiseSeparableConv(nn.Module):\n    def __init__(self, in_ch, out_ch, use_dropout=False, p_drop=0.1):\n        super().__init__()\n        self.depth = ConvBNReLU(in_ch, in_ch, k=3, s=1, p=1,\n                                groups=in_ch, use_dropout=use_dropout, p_drop=p_drop)\n        self.point = ConvBNReLU(in_ch, out_ch, k=1, s=1, p=0,\n                                use_dropout=use_dropout, p_drop=p_drop)\n    def forward(self, x):\n        x = self.depth(x)\n        x = self.point(x)\n        return x\n\nclass SelfAttentionBlock(nn.Module):\n    def __init__(self, dim, num_heads=8, dropout=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n        self.proj = nn.Linear(dim, dim)\n        self.norm = nn.LayerNorm(dim)\n        self.drop = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        b, c, h, w = x.shape\n        n = h * w\n        x_flat = x.permute(0, 2, 3, 1).reshape(b, n, c)\n        x_norm = self.norm(x_flat)\n        q, k, v = self.qkv(x_norm).chunk(3, dim=-1)\n        q = q.view(b, n, self.num_heads, c // self.num_heads).transpose(1, 2)\n        k = k.view(b, n, self.num_heads, c // self.num_heads).transpose(1, 2)\n        v = v.view(b, n, self.num_heads, c // self.num_heads).transpose(1, 2)\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        out = attn @ v\n        out = out.transpose(1, 2).reshape(b, n, c)\n        out = self.drop(self.proj(out))\n        out = out + x_flat\n        out = out.reshape(b, h, w, c).permute(0, 3, 1, 2)\n        return out\n\nclass GlobalContextBlock(nn.Module):\n    def __init__(self, in_channels, reduction=4):\n        super().__init__()\n        self.attention = nn.Conv2d(in_channels, 1, kernel_size=1)\n        self.transform = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1, bias=False),\n        )\n    \n    def forward(self, x):\n        B, C, H, W = x.shape\n        attn = self.attention(x)\n        attn = attn.view(B, 1, -1)\n        attn = torch.softmax(attn, dim=-1)\n        attn = attn.view(B, 1, H, W)\n        context = (x * attn).sum(dim=[2, 3], keepdim=True)\n        out = self.transform(context)\n        return x + out\n\nclass ConvNeXtTinyEncoder(nn.Module):\n    def __init__(self, pretrained=False):\n        super().__init__()\n        try:\n            backbone = torchvision.models.convnext_tiny(weights=\"DEFAULT\" if pretrained else None)\n        except TypeError:\n            backbone = torchvision.models.convnext_tiny(pretrained=pretrained)\n        self.stem = backbone.features[0]   # /4\n        self.stage1 = backbone.features[1] # 1/4,  96C\n        self.stage2 = backbone.features[2] # 1/8,  192C\n        self.stage3 = backbone.features[3] # 1/16, 384C\n        self.stage4 = backbone.features[4] # 1/32, 768C\n    \n    def forward(self, x):\n        x = self.stem(x)\n        e1 = self.stage1(x)      # [B,96,H/4,W/4]\n        e2 = self.stage2(e1)     # [B,192,H/8,W/8]\n        e3 = self.stage3(e2)     # [B,384,H/16,W/16]\n        e4 = self.stage4(e3)     # [B,768,H/32,W/32]\n        return e1, e2, e3, e4\n\nclass Decoder(nn.Module):\n    def __init__(self, encoder_channels=(96, 192, 192, 384),\n                 decoder_channels=(256, 96)):\n        super().__init__()\n        c1, c2, c3, c4 = encoder_channels\n        d1, d2 = decoder_channels\n  \n        self.use_gc = True\n        self.use_mhsa = True\n        self.use_aux = True\n        \n        self.gc_e4 = GlobalContextBlock(c4)\n        self.gc_e3 = GlobalContextBlock(c3)\n        self.att_block = SelfAttentionBlock(c4, num_heads=8, dropout=0.1)\n        self.up3 = nn.ConvTranspose2d(c4, c3, kernel_size=2, stride=2)\n        self.dec3 = DepthwiseSeparableConv(c3 + c3 + c2, d1, use_dropout=True)  # 384+384+192=960 -> 256\n        self.up2 = nn.ConvTranspose2d(d1, 128, kernel_size=2, stride=2)\n        self.dec2 = DepthwiseSeparableConv(128 + c1, d2, use_dropout=True)  # 128+96=224 -> 96\n        self.out_ch = d2\n        self.aux_head_16 = nn.Conv2d(d1, NUM_CLASSES, kernel_size=1)\n        self.aux_head_8  = nn.Conv2d(d2, NUM_CLASSES, kernel_size=1)\n    \n    def forward(self, e1, e2, e3, e4):\n        if self.use_gc:\n            e4 = self.gc_e4(e4)\n            e3 = self.gc_e3(e3)\n        if self.use_mhsa:\n            e4 = self.att_block(e4)\n        \n        x = self.up3(e4)                    # 768->384\n        x = torch.cat([x, e3, e2], dim=1)   # 384+384+192=960\n        x = self.dec3(x)                    # 960->256\n        aux_16 = self.aux_head_16(x) if self.use_aux else None\n        \n        x = self.up2(x)                     # 256->128\n        x = torch.cat([x, e1], dim=1)       # 128+96=224\n        x = self.dec2(x)                    # 224->96\n        aux_8 = self.aux_head_8(x) if self.use_aux else None\n        \n        return x, aux_16, aux_8\n\nclass DetailBranch(nn.Module):\n    def __init__(self, in_ch=3, out_ch=96):\n        super().__init__()\n        self.down = nn.Sequential(\n            ConvBNReLU(in_ch, 32, k=3, s=2, p=1),   # 1/2\n            ConvBNReLU(32, 64, k=3, s=2, p=1),      # 1/4\n        )\n        self.block = DepthwiseSeparableConv(64, out_ch, use_dropout=True)\n    \n    def forward(self, x):\n        x = self.down(x)\n        x = self.block(x)\n        return x\n\nclass SegmentationModel(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super().__init__()\n        self.encoder = ConvNeXtTinyEncoder(pretrained=True)\n        self.decoder = Decoder(\n            encoder_channels=(96, 192, 192, 384),\n            decoder_channels=(256, 96)\n        )\n        self.use_detail = True\n        self.use_boundary = True\n        self.detail_branch = DetailBranch(in_ch=3, out_ch=96)\n        self.fuse = DepthwiseSeparableConv(self.decoder.out_ch + 96, 96, use_dropout=True)\n        self.seg_head = nn.Conv2d(96, num_classes, kernel_size=1)\n        self.boundary_head = nn.Conv2d(96, 1, kernel_size=1)\n    \n    def forward(self, x):\n        e1, e2, e3, e4 = self.encoder(x)\n        dec_1_4, aux_16, aux_8 = self.decoder(e1, e2, e3, e4)\n        \n        if self.use_detail:\n            detail = self.detail_branch(x)\n            fused = torch.cat([dec_1_4, detail], dim=1)\n        else:\n            fused = dec_1_4\n        \n        fused = self.fuse(fused)\n        seg_logits_1_4 = self.seg_head(fused)\n        \n        if self.use_boundary:\n            boundary_logits_1_4 = self.boundary_head(fused)\n        else:\n            boundary_logits_1_4 = None\n        \n        H, W = x.shape[2], x.shape[3]\n        seg_logits = F.interpolate(seg_logits_1_4, size=(H, W), mode=\"bilinear\")\n        \n        if boundary_logits_1_4 is not None:\n            boundary_logits = F.interpolate(boundary_logits_1_4, size=(H, W), mode=\"bilinear\")\n        else:\n            boundary_logits = None\n        \n        if aux_16 is not None:\n            aux_16 = F.interpolate(aux_16, size=(H, W), mode=\"bilinear\")\n        if aux_8 is not None:\n            aux_8 = F.interpolate(aux_8, size=(H, W), mode=\"bilinear\")\n        \n        return {\n            \"logits\": seg_logits,\n            \"aux_16\": aux_16,\n            \"aux_8\": aux_8,\n            \"boundary_logits\": boundary_logits\n        }\n\ndef load_model(model_path):\n    print(f\"Loading model from: {model_path}\")\n\n    model = SegmentationModel(num_classes=NUM_CLASSES)\n\n    checkpoint = torch.load(model_path, map_location=DEVICE)\n    state_dict = checkpoint[\"model_state\"]\n\n\n    state_dict = {\n        k: v for k, v in state_dict.items()\n        if not k.endswith(\"total_ops\") and not k.endswith(\"total_params\")\n    }\n\n    model.load_state_dict(state_dict, strict=True)\n    model.to(DEVICE)\n    model.eval()\n\n    print(f\"Model loaded successfully. Best mIoU: {checkpoint.get('best_mIoU', 'N/A')}\")\n    print(f\"Model device: {next(model.parameters()).device}\")\n\n    return model\n\ndef build_legend(class_ids):\n    handles = []\n    for cid in sorted(class_ids):\n        if cid in CLASS_INFO:\n            name, color = CLASS_INFO[cid]\n            handles.append(\n                Patch(facecolor=np.array(color) / 255.0, label=name)\n            )\n    return handles\n\ndef preprocess_image(image_path):\n\n    img = Image.open(image_path).convert(\"RGB\")\n    orig_size = img.size  # (width, height)\n    \n    mean = (0.485, 0.456, 0.406)\n    std = (0.229, 0.224, 0.225)\n    \n    transform = transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=mean, std=std),\n    ])\n    \n    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n    return img_tensor, orig_size, img\n\ndef decode_segmap(mask_np):\n    h, w = mask_np.shape\n    rgb = COLOR_MAP[mask_np.flatten()].reshape(h, w, 3)\n    return rgb\n\ndef load_ground_truth(gt_path, image_size):\n    if not os.path.exists(gt_path):\n        return None\n\n    gt = Image.open(gt_path).convert(\"L\")\n    gt = gt.resize(image_size, Image.NEAREST)\n    gt_np = np.array(gt, dtype=np.int64)\n\n    # ðŸ”¥ CRITICAL: replicate training preprocessing\n    gt_np[gt_np == 8] = 1      # playground â†’ background\n    gt_np[gt_np == 0] = 0      # ignored stays ignored\n\n    return gt_np\n\n\ndef calculate_metrics(pred_mask, gt_mask):\n    if gt_mask is None:\n        return None\n    \n\n    pred_flat = pred_mask.flatten()\n    gt_flat = gt_mask.flatten()\n    \n\n    valid_mask = gt_flat != IGNORE_INDEX\n    pred_flat = pred_flat[valid_mask]\n    gt_flat = gt_flat[valid_mask]\n    \n    metrics = {}\n\n    accuracy = np.sum(pred_flat == gt_flat) / len(pred_flat)\n    metrics['accuracy'] = accuracy\n    \n    classes_in_gt = np.unique(gt_flat)\n    classes_in_gt = classes_in_gt[\n        (classes_in_gt != IGNORE_INDEX) &\n        (classes_in_gt != 8)\n    ]\n\n\n    metrics['per_class'] = {}\n    metrics['present_classes'] = classes_in_gt.tolist()\n    \n    for cls in classes_in_gt:\n        tp = np.sum((pred_flat == cls) & (gt_flat == cls))\n        fp = np.sum((pred_flat == cls) & (gt_flat != cls))\n        fn = np.sum((pred_flat != cls) & (gt_flat == cls))\n        \n        precision = tp / (tp + fp + 1e-10)\n        recall = tp / (tp + fn + 1e-10)\n        iou = tp / (tp + fp + fn + 1e-10)\n        f1 = 2 * precision * recall / (precision + recall + 1e-10)\n        \n        metrics['per_class'][int(cls)] = {\n            'precision': precision,\n            'recall': recall,\n            'iou': iou,\n            'f1': f1,\n            'support': np.sum(gt_flat == cls),\n            'predicted_count': np.sum(pred_flat == cls)\n        }\n    \n    if len(classes_in_gt) > 0:\n        ious = [metrics['per_class'][int(cls)]['iou'] for cls in classes_in_gt]\n        metrics['mean_iou'] = np.mean(ious)\n    else:\n        metrics['mean_iou'] = 0\n    \n    classes_in_pred = np.unique(pred_flat)\n    metrics['predicted_classes'] = classes_in_pred.tolist()\n\n    fp_classes = [int(c) for c in classes_in_pred if c not in classes_in_gt and c != IGNORE_INDEX]\n    metrics['false_positive_classes'] = fp_classes\n    fn_classes = [int(c) for c in classes_in_gt if c not in classes_in_pred]\n    metrics['false_negative_classes'] = fn_classes\n    \n    return metrics\n\ndef create_comparison_plot(original_img, pred_mask, gt_mask, metrics, image_name):\n    \"\"\"Create comparison plot with metrics\"\"\"\n\n    pred_color = decode_segmap(pred_mask)\n    gt_color = decode_segmap(gt_mask) if gt_mask is not None else None\n    \n\n    overlay = 0.6 * np.array(original_img).astype(np.float32) / 255.0 + 0.4 * pred_color.astype(np.float32) / 255.0\n    overlay = np.clip(overlay * 255, 0, 255).astype(np.uint8)\n    \n    if gt_mask is not None:\n        error_mask = np.zeros_like(pred_color)\n \n        error_mask[(pred_mask != gt_mask) & (gt_mask != IGNORE_INDEX)] = [255, 0, 0]\n \n        error_mask[(pred_mask != gt_mask) & (pred_mask != IGNORE_INDEX)] = [0, 0, 255]\n        error_mask[gt_mask == IGNORE_INDEX] = 0  # Ignore background\n    \n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    axes = axes.flatten()\n    \n\n    axes[0].imshow(original_img)\n    axes[0].set_title(f\"Original Image\\n{image_name}\", fontsize=12)\n    axes[0].axis('off')\n    \n\n    if gt_mask is not None:\n        axes[1].imshow(gt_color)\n    \n        gt_ids = np.unique(gt_mask)\n        gt_names = [CLASS_NAMES[c] for c in gt_ids if c < len(CLASS_NAMES)]\n    \n        axes[1].set_title(\"Ground Truth\", fontsize=12, color='green')\n\n        axes[1].axis('off')\n    else:\n        axes[1].text(0.5, 0.5, \"Ground Truth\\nNot Available\",\n                     ha='center', va='center', fontsize=12)\n        axes[1].axis('off')\n\n    \n\n    axes[2].imshow(pred_color)\n    \n    pred_ids = np.unique(pred_mask)\n    pred_names = [CLASS_NAMES[c] for c in pred_ids if c < len(CLASS_NAMES)]\n    \n    axes[2].set_title(\"Prediction\", fontsize=12, color='blue')\n\n    axes[2].axis('off')\n\n    legend_classes = set(pred_ids)\n    \n    if gt_mask is not None:\n        legend_classes.update(np.unique(gt_mask))\n    \n    legend_handles = build_legend(legend_classes)\n    \n    axes[2].legend(\n        handles=legend_handles,\n        loc=\"lower right\",\n        fontsize=9,\n        frameon=True\n    )\n\n\n\n    axes[3].imshow(overlay)\n    axes[3].set_title(\"Prediction Overlay\", fontsize=12)\n    axes[3].axis('off')\n    \n\n    if gt_mask is not None:\n        axes[4].imshow(error_mask)\n        axes[4].set_title(\"Error Map\\nRed: FP, Blue: FN\", fontsize=12)\n        axes[4].axis('off')\n    else:\n        axes[4].axis('off')\n    \n\n    axes[5].axis('off')\n    if metrics is not None:\n        metrics_text = f\"Overall Metrics:\\n\"\n        metrics_text += f\"Accuracy: {metrics['accuracy']:.3f}\\n\"\n        metrics_text += f\"mIoU: {metrics['mean_iou']:.3f}\\n\\n\"\n        \n        metrics_text += \"Classes in GT: {}\\n\".format(metrics['present_classes'])\n        metrics_text += \"Classes in Pred: {}\\n\".format(metrics['predicted_classes'])\n        \n        if metrics['false_positive_classes']:\n            metrics_text += f\"FP Classes: {metrics['false_positive_classes']}\\n\"\n        if metrics['false_negative_classes']:\n            metrics_text += f\"FN Classes: {metrics['false_negative_classes']}\\n\"\n        \n        metrics_text += \"\\nPer-Class IoU:\\n\"\n        for cls in sorted(metrics['per_class'].keys()):\n            iou = metrics['per_class'][cls]['iou']\n            support = metrics['per_class'][cls]['support']\n            pred_count = metrics['per_class'][cls]['predicted_count']\n            metrics_text += f\"Class {cls}: {iou:.3f} (GT:{support}, Pred:{pred_count})\\n\"\n        \n        axes[5].text(0.1, 0.95, metrics_text, fontsize=9, \n                    verticalalignment='top', family='monospace')\n    else:\n        axes[5].text(0.5, 0.5, \"No metrics available\\n(Ground truth missing)\", \n                    ha='center', va='center', fontsize=12)\n    \n    plt.tight_layout()\n    return fig\n\ndef save_predictions(image_path, pred_mask, gt_mask, color_mask, overlay, \n                     comparison_fig, output_dir, metrics):\n    \"\"\"Save all prediction outputs\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    base_name = os.path.splitext(os.path.basename(image_path))[0]\n    \n\n    mask_path = os.path.join(output_dir, f\"{base_name}_pred_mask.png\")\n    Image.fromarray(color_mask).save(mask_path)\n    \n\n    overlay_path = os.path.join(output_dir, f\"{base_name}_overlay.png\")\n    Image.fromarray(overlay).save(overlay_path)\n    \n\n    pred_path = os.path.join(output_dir, f\"{base_name}_pred.png\")\n    Image.fromarray(pred_mask.astype(np.uint8)).save(pred_path)\n \n    if gt_mask is not None:\n        gt_path = os.path.join(output_dir, f\"{base_name}_gt.png\")\n        Image.fromarray(gt_mask.astype(np.uint8)).save(gt_path)\n        \n\n        gt_color = decode_segmap(gt_mask)\n        gt_color_path = os.path.join(output_dir, f\"{base_name}_gt_color.png\")\n        Image.fromarray(gt_color).save(gt_color_path)\n    \n\n    comparison_path = os.path.join(output_dir, f\"{base_name}_comparison.png\")\n    comparison_fig.savefig(comparison_path, dpi=150, bbox_inches='tight')\n    plt.close(comparison_fig)\n    \n\n    if metrics is not None:\n        metrics_path = os.path.join(output_dir, f\"{base_name}_metrics.txt\")\n        with open(metrics_path, 'w') as f:\n            f.write(f\"Image: {image_path}\\n\")\n            f.write(f\"Overall Accuracy: {metrics['accuracy']:.4f}\\n\")\n            f.write(f\"mIoU (for present classes): {metrics['mean_iou']:.4f}\\n\")\n            f.write(f\"Classes in GT: {metrics['present_classes']}\\n\")\n            f.write(f\"Classes in Prediction: {metrics['predicted_classes']}\\n\")\n            \n            if metrics['false_positive_classes']:\n                f.write(f\"False Positive Classes (predicted but not in GT): {metrics['false_positive_classes']}\\n\")\n            if metrics['false_negative_classes']:\n                f.write(f\"False Negative Classes (in GT but not predicted): {metrics['false_negative_classes']}\\n\")\n            \n            f.write(\"\\nPer-Class Metrics:\\n\")\n            f.write(\"Class | Precision | Recall   | IoU      | F1-Score | GT Pixels | Pred Pixels\\n\")\n            f.write(\"-\" * 80 + \"\\n\")\n            \n            for cls in sorted(metrics['per_class'].keys()):\n                m = metrics['per_class'][cls]\n                f.write(f\"{cls:5d} | {m['precision']:.4f}   | {m['recall']:.4f}   | \"\n                       f\"{m['iou']:.4f}   | {m['f1']:.4f}   | {m['support']:9d} | {m['predicted_count']:11d}\\n\")\n    \n    saved_paths = {\n        \"mask_path\": mask_path,\n        \"overlay_path\": overlay_path,\n        \"pred_path\": pred_path,\n        \"comparison_path\": comparison_path,\n    }\n    \n    if gt_mask is not None:\n        saved_paths[\"gt_path\"] = gt_path\n        saved_paths[\"gt_color_path\"] = gt_color_path\n        saved_paths[\"metrics_path\"] = metrics_path\n    \n    return saved_paths\n\n\ndef predict_single_image(model, image_path, gt_path=None, output_dir=None, return_all=False):\n\n    img_tensor, orig_size, orig_img = preprocess_image(image_path)\n    img_tensor = img_tensor.to(DEVICE)\n\n\n\n    with torch.no_grad():\n        with torch.amp.autocast('cuda', enabled=AMP_ENABLED):\n            start_time = time.perf_counter()\n            outputs = model(img_tensor)\n            logits = outputs[\"logits\"]\n            \n\n            probs = F.softmax(logits, dim=1)\n            pred = torch.argmax(logits, dim=1)\n            end_time = time.perf_counter()\n            \n\n\n            pred_np = pred[0].cpu().numpy()  # [H, W]\n            probs_np = probs[0].cpu().numpy()  # [C, H, W]\n        \n\n    \n    inference_time_ms = (end_time - start_time) * 1000\n    print(f\"[Inference Time] Prediction mask computed in {inference_time_ms:.2f} ms\")\n\n    pred_pil = Image.fromarray(pred_np.astype(np.uint8))\n    pred_resized = pred_pil.resize(orig_size, Image.NEAREST)\n    pred_final = np.array(pred_resized)\n\n    gt_final = None\n    if gt_path:\n        gt_final = load_ground_truth(gt_path, orig_size)\n    \n\n    metrics = None\n    if gt_final is not None:\n        metrics = calculate_metrics(pred_final, gt_final)\n    \n\n    color_mask = decode_segmap(pred_final)\n    \n\n    comparison_fig = create_comparison_plot(\n        orig_img, pred_final, gt_final, metrics, \n        os.path.basename(image_path)\n    )\n    \n\n    orig_np = np.array(orig_img)\n    overlay = 0.6 * orig_np.astype(np.float32) / 255.0 + 0.4 * color_mask.astype(np.float32) / 255.0\n    overlay = np.clip(overlay * 255, 0, 255).astype(np.uint8)\n    \n   \n    saved_paths = None\n    if output_dir:\n        saved_paths = save_predictions(\n            image_path, pred_final, gt_final, color_mask, overlay, \n            comparison_fig, output_dir, metrics\n        )\n        print(f\"âœ“ Saved predictions for {os.path.basename(image_path)}\")\n        if metrics:\n            print(f\"  Accuracy: {metrics['accuracy']:.3f}, mIoU: {metrics['mean_iou']:.3f}\")\n    \n    if return_all:\n        return {\n            \"prediction\": pred_final,\n            \"ground_truth\": gt_final,\n            \"probabilities\": probs_np,\n            \"color_mask\": color_mask,\n            \"overlay\": overlay,\n            \"metrics\": metrics,\n            \"saved_paths\": saved_paths\n        }\n    else:\n        return pred_final, gt_final, metrics\n\ndef predict_batch(model, image_dir, gt_dir=None, output_dir=None, batch_size=1):\n    \"\"\"Run inference on all images in a directory with optional ground truth\"\"\"\n    import glob\n    \n    # Find all images\n    image_extensions = [\"*.png\", \"*.jpg\", \"*.jpeg\", \"*.tif\", \"*.tiff\"]\n    image_paths = []\n    for ext in image_extensions:\n        image_paths.extend(glob.glob(os.path.join(image_dir, ext)))\n    \n    print(f\"Found {len(image_paths)} images in {image_dir}\")\n    \n    # Process each image\n    all_metrics = []\n    for i, img_path in enumerate(image_paths):\n        print(f\"\\nProcessing {i+1}/{len(image_paths)}: {os.path.basename(img_path)}\")\n        \n\n        gt_path = None\n        if gt_dir:\n   \n            base_name = os.path.splitext(os.path.basename(img_path))[0]\n            possible_gt_names = [\n                f\"{base_name}.png\",\n                f\"{base_name}.tif\",\n                f\"{base_name}.jpg\",\n                f\"{base_name}_mask.png\",\n                f\"{base_name}_label.png\",\n                base_name.replace(\"image\", \"mask\"),\n                base_name.replace(\"img\", \"mask\"),\n            ]\n            \n            for gt_name in possible_gt_names:\n                gt_test_path = os.path.join(gt_dir, gt_name)\n                if os.path.exists(gt_test_path):\n                    gt_path = gt_test_path\n                    break\n        \n\n        pred, gt, metrics = predict_single_image(\n            model, img_path, gt_path, output_dir, return_all=False\n        )\n        \n        if metrics:\n            all_metrics.append(metrics)\n            print(f\"  Accuracy: {metrics['accuracy']:.3f}, mIoU: {metrics['mean_iou']:.3f}\")\n    \n    # Calculate average metrics if available\n    if all_metrics:\n        print(\"\\n\" + \"=\"*60)\n        print(\"AGGREGATE METRICS\")\n        print(\"=\"*60)\n        \n        avg_accuracy = np.mean([m['accuracy'] for m in all_metrics])\n        avg_miou = np.mean([m['mean_iou'] for m in all_metrics])\n        \n        print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n        print(f\"Average mIoU: {avg_miou:.4f}\")\n        \n\n        print(\"\\nPer-Class Average IoU (only for classes that appear):\")\n        all_classes = set()\n        for m in all_metrics:\n            all_classes.update(m['per_class'].keys())\n        \n        class_stats = {}\n        for cls in sorted(all_classes):\n            class_ious = []\n            class_supports = []\n            for m in all_metrics:\n                if cls in m['per_class']:\n                    class_ious.append(m['per_class'][cls]['iou'])\n                    class_supports.append(m['per_class'][cls]['support'])\n            \n            if class_ious:\n                avg_iou = np.mean(class_ious)\n                total_support = np.sum(class_supports)\n                num_images_with_class = len(class_ious)\n                class_stats[cls] = {\n                    'avg_iou': avg_iou,\n                    'total_support': total_support,\n                    'num_images': num_images_with_class\n                }\n                print(f\"  Class {cls}: {avg_iou:.4f} (appears in {num_images_with_class} images, {total_support} total pixels)\")\n        \n       \n        if output_dir:\n            agg_metrics_path = os.path.join(output_dir, \"aggregate_metrics.txt\")\n            with open(agg_metrics_path, 'w') as f:\n                f.write(f\"Total Images: {len(all_metrics)}\\n\")\n                f.write(f\"Average Accuracy: {avg_accuracy:.4f}\\n\")\n                f.write(f\"Average mIoU: {avg_miou:.4f}\\n\\n\")\n                \n                f.write(\"Per-Class Statistics:\\n\")\n                f.write(\"Class | Avg IoU | # Images | Total Pixels\\n\")\n                f.write(\"-\" * 50 + \"\\n\")\n                \n                for cls, stats in class_stats.items():\n                    f.write(f\"{cls:5d} | {stats['avg_iou']:.4f}  | {stats['num_images']:8d} | {stats['total_support']:12d}\\n\")\n            \n            print(f\"\\nâœ“ Aggregate metrics saved to: {agg_metrics_path}\")\n    \n    return all_metrics\n\n\ndef run_example_inference():\n    \"\"\"Example showing how to use the inference functions with ground truth\"\"\"\n    print(\"=\" * 60)\n    print(\"EARTHVQA INFERENCE SCRIPT WITH GROUND TRUTH COMPARISON\")\n    print(\"=\" * 60)\n    \n    # === CONFIGURE THESE PATHS ===\n    MODEL_PATH = \"/kaggle/input/loveda-test-wrong-original/best_model_earthvqa.pth\"  # Your trained model\n    IMAGE_PATH = \"/kaggle/input/loveda-test-wrong-original/Val/Val/images_png/2544.png\"  # Test image\n    GT_PATH = \"/kaggle/input/loveda-test-wrong-original/Val/Val/masks_png/2544.png\"  # Ground truth\n    OUTPUT_DIR = \"/kaggle/working/inference_results\"  # Where to save results\n    \n\n    # IMAGE_DIR = \"/kaggle/input/loveda-test-wrong-original/Test/Test/images_png\"\n    # GT_DIR = \"/kaggle/input/loveda-test-wrong-original/Test/Test/masks_png\"\n    \n    print(f\"Model path: {MODEL_PATH}\")\n    print(f\"Image path: {IMAGE_PATH}\")\n    print(f\"Ground truth path: {GT_PATH}\")\n    print(f\"Output directory: {OUTPUT_DIR}\")\n    print(f\"Device: {DEVICE}\")\n    print(\"-\" * 60)\n    \n\n    model = load_model(MODEL_PATH)\n    \n    print(\"\\nRunning inference with ground truth comparison...\")\n    result = predict_single_image(model, IMAGE_PATH, GT_PATH, OUTPUT_DIR, return_all=True)\n    \n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"INFERENCE RESULTS\")\n    print(\"=\" * 60)\n    print(f\"Prediction shape: {result['prediction'].shape}\")\n    print(f\"Unique classes predicted: {np.unique(result['prediction'])}\")\n    \n    if result['ground_truth'] is not None:\n        print(f\"Unique classes in ground truth: {np.unique(result['ground_truth'])}\")\n    \n    if result['metrics']:\n        print(f\"\\nEvaluation Metrics:\")\n        print(f\"  Overall Accuracy: {result['metrics']['accuracy']:.4f}\")\n        print(f\"  Mean IoU (for present classes): {result['metrics']['mean_iou']:.4f}\")\n        \n        print(f\"\\nClasses in Ground Truth: {result['metrics']['present_classes']}\")\n        print(f\"Classes in Prediction: {result['metrics']['predicted_classes']}\")\n        \n        if result['metrics']['false_positive_classes']:\n            print(f\"False Positive Classes (predicted but not in GT): {result['metrics']['false_positive_classes']}\")\n        if result['metrics']['false_negative_classes']:\n            print(f\"False Negative Classes (in GT but not predicted): {result['metrics']['false_negative_classes']}\")\n        \n        print(\"\\nPer-Class IoU (only for classes present in ground truth):\")\n        for cls in sorted(result['metrics']['per_class'].keys()):\n            iou = result['metrics']['per_class'][cls]['iou']\n            support = result['metrics']['per_class'][cls]['support']\n            pred_count = result['metrics']['per_class'][cls]['predicted_count']\n            print(f\"  Class {cls}: {iou:.4f} (GT: {support}px, Pred: {pred_count}px)\")\n    \n    if result['saved_paths']:\n        print(\"\\nSaved files:\")\n        for key, path in result['saved_paths'].items():\n            print(f\"  {key}: {path}\")\n    \n\n    print(\"\\nDisplaying comparison results...\")\n    plt.figure(figsize=(15, 10))\n    img = plt.imread(result['saved_paths']['comparison_path'])\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title(\"Inference Results Comparison\", fontsize=14, pad=20)\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nâœ“ Inference complete! Results saved to: {OUTPUT_DIR}\")\n    \n    return model, result\n\n\nif __name__ == \"__main__\":\n    run_example_inference()\n    \n    # For batch processing with ground truth:\n    # model = load_model(\"/kaggle/working/best_model_earthvqa.pth\")\n    # results = predict_batch(model, \n    #                         \"/kaggle/input/loveda-test-wrong-original/Test/Test/images_png\",\n    #                         \"/kaggle/input/loveda-test-wrong-original/Test/Test/masks_png\",\n    #                         \"/kaggle/working/batch_results\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}