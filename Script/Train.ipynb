{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\nimport random\nfrom typing import Dict, Tuple\n\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision\nfrom torchvision import transforms\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nSEED = 42\n\n# name the bad images\nBAD_IMAGE_BASENAMES = {\n    \"1004_r270.png\",\n}\n\n\nRESUME_TRAINING = True      \nSAVE_EVERY_EPOCH = False     \n\n# ablations\n\nABLATE_GC = True           # GlobalContextBlock (GC) on e3/e4\nABLATE_MHSA = True        # SelfAttentionBlock on e4\nABLATE_DETAIL = True       # Detail branch at 1/4\nABLATE_AUX = True          # Aux heads (aux_16, aux_8)\nABLATE_BOUNDARY = True     # Boundary prediction + loss\nABLATE_LOVASZ = True       # Lovasz-Softmax in main loss\n\n\nROOT_DIR = \"/kaggle/input/loveda\" \nTRAIN_DIR = os.path.join(ROOT_DIR, \"Train/Train\")\nVAL_DIR = os.path.join(ROOT_DIR, \"Val/Val\")\nTEST_DIR = os.path.join(ROOT_DIR, \"Test/Test\")\n\nTRAIN_IMG_DIR = os.path.join(TRAIN_DIR, \"images_png\")\nTRAIN_MASK_DIR = os.path.join(TRAIN_DIR, \"masks_png\")\nVAL_IMG_DIR = os.path.join(VAL_DIR, \"images_png\")\nVAL_MASK_DIR = os.path.join(VAL_DIR, \"masks_png\")\nTEST_IMG_DIR = os.path.join(TEST_DIR, \"images_png\")\n\n# output\nWORK_DIR = \"/kaggle/working\"\nos.makedirs(WORK_DIR, exist_ok=True)\nBEST_MODEL_PATH = os.path.join(WORK_DIR, \"best_model_earthvqa.pth\")\nPLOT_PATH = os.path.join(WORK_DIR, \"train_curves.png\")\nTEST_OUT_DIR = os.path.join(WORK_DIR, \"test_inference\")\nos.makedirs(TEST_OUT_DIR, exist_ok=True)\n\nNUM_CLASSES = 8             # labels: 0 to 8, 0 = background (ignored)\nIGNORE_INDEX = 0            # background ignored in loss\n\nBATCH_SIZE = 4             \nNUM_EPOCHS = 25\nEARLY_STOP_PATIENCE = 3\n\nLR = 1e-4\nWEIGHT_DECAY = 1e-5\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nAMP_ENABLED = (DEVICE == \"cuda\")\n\nIMG_SIZE = 512  \n\nLAMBDA_MAIN = 1.0\nLAMBDA_AUX = 0.3\nLAMBDA_BOUNDARY = 0.5\n\n# indices: 0  1  2  3  4  5  6  7  8\nCOLOR_MAP = np.array([\n    [0, 0, 0],        # 0\n    [128, 0, 0],       # 1\n    [0, 128, 0],       # 2\n    [128, 128, 0],      # 3\n    [0, 0, 128],      # 4\n    [128, 0, 128],      # 5\n    [0, 128, 128],     # 6\n    [128, 128, 128],  # 7\n    [64, 0, 0],        # 8   merged with class 1 background\n], dtype=np.uint8)\n\n\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(SEED)\n\n\n\nclass EarthVQADataset(Dataset):\n    def __init__(self, img_dir: str, mask_dir: str, img_size: int = 1024):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n\n        all_img_paths = sorted([\n            os.path.join(img_dir, f) for f in os.listdir(img_dir)\n            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".tif\"))\n        ])\n\n        self.img_paths = []\n        self.mask_paths = []\n\n        for p in all_img_paths:\n            fname = os.path.basename(p)\n            if fname in BAD_IMAGE_BASENAMES:\n                print(f\"[INFO] Skipping corrupted image and mask: {fname}\")\n                continue\n\n            self.img_paths.append(p)\n            self.mask_paths.append(os.path.join(mask_dir, fname))\n\n        self.img_size = img_size\n\n        self.mean = (0.485, 0.456, 0.406)\n        self.std = (0.229, 0.224, 0.225)\n\n        self.img_tf = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=self.mean, std=self.std),\n        ])\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx: int):\n        img_path = self.img_paths[idx]\n        mask_path = self.mask_paths[idx]\n\n        img = Image.open(img_path).convert(\"RGB\")\n        mask = Image.open(mask_path).convert(\"L\")  # single-channel labels\n\n        img_t = self.img_tf(img)\n\n        mask_resized = transforms.functional.resize(\n            mask, (self.img_size, self.img_size), interpolation=Image.NEAREST\n        )\n        mask_arr = np.array(mask_resized, dtype=np.int64)\n\n# remapping class 8 to 1\n        mask_arr[mask_arr == 8] = 1   # playground to background\n\n        \n        mask_t = torch.from_numpy(mask_arr)\n\n\n        return img_t, mask_t\n\n\nclass EarthVQATestDataset(Dataset):\n    \"\"\"\n    Test dataset with only images (no masks).\n    \"\"\"\n    def __init__(self, img_dir: str, img_size: int = 1024):\n        self.img_dir = img_dir\n        self.img_paths = sorted([\n            os.path.join(img_dir, f) for f in os.listdir(img_dir)\n            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".tif\"))\n        ])\n        self.img_size = img_size\n\n        self.mean = (0.485, 0.456, 0.406)\n        self.std = (0.229, 0.224, 0.225)\n\n        self.img_tf = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=self.mean, std=self.std),\n        ])\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx: int):\n        img_path = self.img_paths[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        img_t = self.img_tf(img)\n        return img_t, os.path.basename(img_path)\n\n\n# Model archi\n\nclass ConvBNReLU(nn.Module):\n    def __init__(self, in_ch, out_ch, k=3, s=1, p=1, groups=1, use_dropout=False, p_drop=0.1):\n        super().__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, k, s, p, bias=False, groups=groups)\n        self.bn = nn.BatchNorm2d(out_ch)\n        self.relu = nn.ReLU(inplace=True)\n        self.drop = nn.Dropout2d(p_drop) if use_dropout else nn.Identity()\n\n        nn.init.kaiming_normal_(self.conv.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.drop(x)\n        return x\n\n\nclass DepthwiseSeparableConv(nn.Module):\n    def __init__(self, in_ch, out_ch, use_dropout=False, p_drop=0.1):\n        super().__init__()\n        self.depth = ConvBNReLU(in_ch, in_ch, k=3, s=1, p=1,\n                                groups=in_ch, use_dropout=use_dropout, p_drop=p_drop)\n        self.point = ConvBNReLU(in_ch, out_ch, k=1, s=1, p=0,\n                                use_dropout=use_dropout, p_drop=p_drop)\n\n    def forward(self, x):\n        x = self.depth(x)\n        x = self.point(x)\n        return x\n\n\nclass SelfAttentionBlock(nn.Module):\n    \n    def __init__(self, dim, num_heads=8, dropout=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n        self.proj = nn.Linear(dim, dim)\n        self.norm = nn.LayerNorm(dim)\n        self.drop = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # x: [B, C, H, W]\n        b, c, h, w = x.shape\n        n = h * w\n\n        #\n        x_flat = x.permute(0, 2, 3, 1).reshape(b, n, c)\n        x_norm = self.norm(x_flat)\n\n        q, k, v = self.qkv(x_norm).chunk(3, dim=-1)\n\n        q = q.view(b, n, self.num_heads, c // self.num_heads).transpose(1, 2)\n        k = k.view(b, n, self.num_heads, c // self.num_heads).transpose(1, 2)\n        v = v.view(b, n, self.num_heads, c // self.num_heads).transpose(1, 2)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        out = attn @ v\n\n        out = out.transpose(1, 2).reshape(b, n, c)\n        out = self.drop(self.proj(out))\n\n        out = out + x_flat\n        out = out.reshape(b, h, w, c).permute(0, 3, 1, 2)\n        return out\n\n\nclass GlobalContextBlock(nn.Module):\n    \n    def __init__(self, in_channels, reduction=4):\n        super().__init__()\n\n       \n        self.attention = nn.Conv2d(in_channels, 1, kernel_size=1)\n\n        self.transform = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1, bias=False),\n        )\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n\n      \n        attn = self.attention(x)\n        attn = attn.view(B, 1, -1)\n        attn = torch.softmax(attn, dim=-1)\n        attn = attn.view(B, 1, H, W)\n\n        context = (x * attn).sum(dim=[2, 3], keepdim=True)\n        out = self.transform(context)\n        return x + out\n\n\nclass ConvNeXtTinyEncoder(nn.Module):\n    \"\"\"\n    ConvNeXt-Tiny encoder.\n    Returns features at:\n        e1: 1/4,   C=96\n        e2: 1/8,   C=192\n        e3: 1/16,  C=384\n        e4: 1/32,  C=768\n    \"\"\"\n    def __init__(self, pretrained: bool = True):\n        super().__init__()\n        try:\n            if pretrained:\n                backbone = torchvision.models.convnext_tiny(weights=\"DEFAULT\")\n            else:\n                backbone = torchvision.models.convnext_tiny(weights=None)\n        except TypeError:\n            backbone = torchvision.models.convnext_tiny(pretrained=pretrained)\n\n        self.stem = backbone.features[0]   # /4\n        self.stage1 = backbone.features[1] # 1/4,  96C\n        self.stage2 = backbone.features[2] # 1/8,  192C\n        self.stage3 = backbone.features[3] # 1/16, 384C\n        self.stage4 = backbone.features[4] # 1/32, 768C\n\n    def forward(self, x):\n        x = self.stem(x)         # [B,96,H/4,W/4]\n        e1 = self.stage1(x)      # [B,96,H/4,W/4]\n        e2 = self.stage2(e1)     # [B,192,H/8,W/8]\n        e3 = self.stage3(e2)     # [B,384,H/16,W/16]\n        e4 = self.stage4(e3)     # [B,768,H/32,W/32]\n        return e1, e2, e3, e4\n\n\nclass Decoder(nn.Module):\n\n    def __init__(self, encoder_channels=(96, 192, 192, 384),\n                 decoder_channels=(256, 96)):\n        super().__init__()\n\n        c1, c2, c3, c4 = encoder_channels\n        d1, d2 = decoder_channels\n\n        # Ablation toggles\n        self.use_gc = ABLATE_GC\n        self.use_mhsa = ABLATE_MHSA\n        self.use_aux = ABLATE_AUX\n\n        self.gc_e4 = GlobalContextBlock(c4)\n        self.gc_e3 = GlobalContextBlock(c3)\n        self.att_block = SelfAttentionBlock(c4, num_heads=8, dropout=0.1)\n\n        self.up3 = nn.ConvTranspose2d(c4, c3, kernel_size=2, stride=2)\n\n        self.dec3 = DepthwiseSeparableConv(576, d1, use_dropout=True)\n\n        self.up2 = nn.ConvTranspose2d(d1, 128, kernel_size=2, stride=2)\n        self.dec2 = DepthwiseSeparableConv(128 + c1, d2, use_dropout=True)\n\n        self.out_ch = d2\n\n        self.aux_head_16 = nn.Conv2d(d1, NUM_CLASSES, kernel_size=1)\n        self.aux_head_8  = nn.Conv2d(d2, NUM_CLASSES, kernel_size=1)\n\n    def forward(self, e1, e2, e3, e4):\n\n        if self.use_gc:\n            e4 = self.gc_e4(e4)\n            e3 = self.gc_e3(e3)\n\n        if self.use_mhsa:\n            e4 = self.att_block(e4)\n\n        x = self.up3(e4)\n        x = torch.cat([x, e3, e2], dim=1)\n        x = self.dec3(x)\n        aux_16 = self.aux_head_16(x) if self.use_aux else None\n\n        x = self.up2(x)\n        x = torch.cat([x, e1], dim=1)\n        x = self.dec2(x)\n        aux_8 = self.aux_head_8(x) if self.use_aux else None\n\n        return x, aux_16, aux_8\n\n\n\nclass DetailBranch(nn.Module):\n\n    def __init__(self, in_ch=3, out_ch=96):\n        super().__init__()\n        self.down = nn.Sequential(\n            ConvBNReLU(in_ch, 32, k=3, s=2, p=1),   # 1/2\n            ConvBNReLU(32, 64, k=3, s=2, p=1),      # 1/4\n        )\n        self.block = DepthwiseSeparableConv(64, out_ch, use_dropout=True)\n\n    def forward(self, x):\n        x = self.down(x)\n        x = self.block(x)\n        return x\n\n\nclass SegmentationModel(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super().__init__()\n\n        self.encoder = ConvNeXtTinyEncoder(pretrained=True)\n\n        self.decoder = Decoder(\n            encoder_channels=(96, 192, 192, 384),\n            decoder_channels=(256, 96)\n        )\n\n        self.use_detail = ABLATE_DETAIL\n        self.use_boundary = ABLATE_BOUNDARY\n\n        self.detail_branch = DetailBranch(in_ch=3, out_ch=96)\n        self.fuse = DepthwiseSeparableConv(self.decoder.out_ch + 96, 96, use_dropout=True)\n\n        self.seg_head = nn.Conv2d(96, num_classes, kernel_size=1)\n        self.boundary_head = nn.Conv2d(96, 1, kernel_size=1)\n\n    def forward(self, x):\n        e1, e2, e3, e4 = self.encoder(x)\n        dec_1_4, aux_16, aux_8 = self.decoder(e1, e2, e3, e4)\n\n        if self.use_detail:\n            detail = self.detail_branch(x)\n            fused = torch.cat([dec_1_4, detail], dim=1)\n        else:\n            fused = dec_1_4\n\n        fused = self.fuse(fused)\n\n        seg_logits_1_4 = self.seg_head(fused)\n\n        if self.use_boundary:\n            boundary_logits_1_4 = self.boundary_head(fused)\n        else:\n            boundary_logits_1_4 = None\n\n        H, W = x.shape[2], x.shape[3]\n        seg_logits = F.interpolate(seg_logits_1_4, size=(H, W), mode=\"bilinear\")\n\n        if boundary_logits_1_4 is not None:\n            boundary_logits = F.interpolate(boundary_logits_1_4, size=(H, W), mode=\"bilinear\")\n        else:\n            boundary_logits = None\n\n        if aux_16 is not None:\n            aux_16 = F.interpolate(aux_16, size=(H, W), mode=\"bilinear\")\n        if aux_8 is not None:\n            aux_8 = F.interpolate(aux_8, size=(H, W), mode=\"bilinear\")\n\n        return {\n            \"logits\": seg_logits,\n            \"aux_16\": aux_16,\n            \"aux_8\": aux_8,\n            \"boundary_logits\": boundary_logits\n        }\n\n\ndef lovasz_grad(gt_sorted):\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.cumsum(0)\n    union = gts + (1 - gt_sorted).cumsum(0)\n    jaccard = 1.0 - intersection / union\n    if gt_sorted.numel() > 1:\n        jaccard[1:] = jaccard[1:] - jaccard[:-1]\n    return jaccard\n\n\ndef lovasz_softmax_flat(probs, labels):\n    C = probs.size(1)\n    losses = []\n    for c in range(C):\n        fg = (labels == c).float()\n        if fg.sum() == 0:\n            continue\n\n        class_pred = probs[:, c]\n        errors = (fg - class_pred).abs()\n\n        errors_sorted, perm = torch.sort(errors, descending=True)\n        fg_sorted = fg[perm]\n\n        grad = lovasz_grad(fg_sorted)\n        losses.append(torch.dot(errors_sorted, grad))\n\n    if len(losses) == 0:\n        return torch.tensor(0., device=probs.device)\n    return sum(losses) / len(losses)\n\n\ndef lovasz_softmax(probs, labels):\n    probs_flat = probs.permute(0, 2, 3, 1).contiguous().view(-1, probs.size(1))\n    labels_flat = labels.view(-1)\n    return lovasz_softmax_flat(probs_flat, labels_flat)\n\n\nclass CombinedLoss(nn.Module):\n \n    def __init__(self,\n                 num_classes=NUM_CLASSES,\n                 ignore_index=IGNORE_INDEX,\n                 lambda_main=1.0,\n                 lambda_aux=0.3,\n                 lambda_boundary=0.5,\n                 lambda_lovasz=1.0):\n\n        super().__init__()\n        self.ce = nn.CrossEntropyLoss(ignore_index=ignore_index)\n        self.ignore_index = ignore_index\n        self.num_classes = num_classes\n\n        self.lambda_main = lambda_main\n        self.lambda_aux = lambda_aux\n        self.lambda_boundary = lambda_boundary\n        self.lambda_lovasz = lambda_lovasz\n\n        self.bce_boundary = nn.BCEWithLogitsLoss()\n\n        self.use_aux = ABLATE_AUX\n        self.use_boundary = ABLATE_BOUNDARY\n        self.use_lovasz = ABLATE_LOVASZ\n\n    def dice_loss(self, logits, targets, eps=1e-6):\n        num_classes = logits.shape[1]\n        probs = F.softmax(logits, dim=1)\n\n        one_hot = F.one_hot(\n            torch.clamp(targets, 0, num_classes - 1),\n            num_classes=num_classes\n        ).permute(0, 3, 1, 2).float()\n\n        # ignore background index\n        ignore_mask = (targets == self.ignore_index).unsqueeze(1)  # [B,1,H,W]\n        one_hot = torch.where(ignore_mask, torch.zeros_like(one_hot), one_hot)\n        probs = torch.where(ignore_mask, torch.zeros_like(probs), probs)\n\n        dims = (0, 2, 3)\n        intersection = torch.sum(probs * one_hot, dims)\n        cardinality = torch.sum(probs + one_hot, dims)\n        dice = (2. * intersection + eps) / (cardinality + eps)\n\n        valid = torch.arange(num_classes, device=logits.device) != self.ignore_index\n        dice = dice[valid]\n\n        return 1. - dice.mean()\n\n\n    def boundary_from_mask(self, mask: torch.Tensor, dilation: int = 1) -> torch.Tensor:\n        \"\"\"\n        mask: [B,H,W] integer labels\n        returns: [B,1,H,W] boundary map {0,1}\n        \"\"\"\n        b, h, w = mask.shape\n        mask = mask.unsqueeze(1).float()  # [B,1,H,W]\n        pad = dilation\n        max_pool = F.max_pool2d(mask, kernel_size=3, stride=1, padding=pad)\n        min_pool = -F.max_pool2d(-mask, kernel_size=3, stride=1, padding=pad)\n        boundary = (max_pool != min_pool).float()\n        return boundary\n\n\n    def forward(self, outputs: Dict[str, torch.Tensor], targets: torch.Tensor):\n\n        logits = outputs[\"logits\"]               # [B,C,H,W]\n        aux_16 = outputs[\"aux_16\"]              # or None\n        aux_8 = outputs[\"aux_8\"]                # or None\n        boundary_logits = outputs[\"boundary_logits\"]  # or None\n\n        loss_ce_main = self.ce(logits, targets)\n        loss_dice_main = self.dice_loss(logits, targets)\n        loss_main = loss_ce_main + loss_dice_main\n\n        if self.use_lovasz:\n            probs = F.softmax(logits, dim=1)\n            loss_lovasz = lovasz_softmax(probs, targets)\n            loss_main = loss_main + self.lambda_lovasz * loss_lovasz\n        else:\n            loss_lovasz = torch.tensor(0.0, device=logits.device)\n\n        if self.use_aux and (aux_16 is not None) and (aux_8 is not None):\n            loss_aux_16 = self.ce(aux_16, targets)\n            loss_aux_8 = self.ce(aux_8, targets)\n            loss_aux = 0.5 * (loss_aux_16 + loss_aux_8)\n        else:\n            loss_aux = torch.tensor(0.0, device=logits.device)\n\n        if self.use_boundary and (boundary_logits is not None):\n            with torch.no_grad():\n                boundary_gt = self.boundary_from_mask(targets)\n            loss_boundary = self.bce_boundary(boundary_logits, boundary_gt)\n        else:\n            loss_boundary = torch.tensor(0.0, device=logits.device)\n\n        total_loss = (\n            self.lambda_main * loss_main +\n            self.lambda_aux * loss_aux +\n            self.lambda_boundary * loss_boundary\n        )\n\n        loss_dict = {\n            \"loss_total\": float(total_loss.item()),\n            \"loss_main\": float(loss_main.item()),\n            \"loss_ce\": float(loss_ce_main.item()),\n            \"loss_dice\": float(loss_dice_main.item()),\n            \"loss_lovasz\": float(loss_lovasz.item()),\n            \"loss_aux\": float(loss_aux.item()),\n            \"loss_boundary\": float(loss_boundary.item()),\n        }\n\n        return total_loss, loss_dict\n\n\n\ndef compute_confusion_matrix(pred: torch.Tensor,\n                             target: torch.Tensor,\n                             num_classes: int,\n                             ignore_index: int) -> np.ndarray:\n    pred = pred.view(-1).cpu().numpy()\n    target = target.view(-1).cpu().numpy()\n    mask = target != ignore_index\n    pred = pred[mask]\n    target = target[mask]\n    cm = np.bincount(\n        num_classes * target.astype(int) + pred.astype(int),\n        minlength=num_classes ** 2\n    ).reshape(num_classes, num_classes)\n    return cm\n\n\ndef metrics_from_confusion_matrix(cm: np.ndarray, ignore_index: int):\n    tp = np.diag(cm)\n    sum_rows = cm.sum(axis=1)\n    sum_cols = cm.sum(axis=0)\n    union = sum_rows + sum_cols - tp\n\n    iou = tp / np.maximum(union, 1e-6)\n    acc = tp / np.maximum(sum_rows, 1e-6)\n\n    num_classes = cm.shape[0]\n    valid = np.arange(num_classes) != ignore_index\n\n    mIoU = np.nanmean(iou[valid])\n    mAcc = np.nanmean(acc[valid])\n\n    pixel_acc = tp.sum() / np.maximum(cm.sum(), 1e-6)\n\n    freq = sum_rows / np.maximum(cm.sum(), 1e-6)\n    fwIoU = (freq[valid] * iou[valid]).sum()\n\n    metrics = {\n        \"mIoU\": float(mIoU),\n        \"pixel_acc\": float(pixel_acc),\n        \"mAcc\": float(mAcc),\n        \"FWIoU\": float(fwIoU),\n    }\n    return metrics, iou, acc\n\n\ndef boundary_f1(pred: torch.Tensor, target: torch.Tensor,\n                ignore_index: int, dilation: int = 1) -> float:\n    def extract_boundary(mask: torch.Tensor) -> torch.Tensor:\n        b, h, w = mask.shape\n        mask = mask.unsqueeze(1).float()\n        pad = dilation\n        max_pool = F.max_pool2d(mask, kernel_size=3, stride=1, padding=pad)\n        min_pool = -F.max_pool2d(-mask, kernel_size=3, stride=1, padding=pad)\n        boundary = (max_pool != min_pool).float()\n        return boundary\n\n    with torch.no_grad():\n        target_mask = target.clone()\n        target_mask[target_mask == ignore_index] = 0\n        pred_mask = pred.clone()\n        pred_mask[target == ignore_index] = 0\n\n        b_gt = extract_boundary(target_mask)\n        b_pred = extract_boundary(pred_mask)\n\n        b_gt = b_gt.view(-1)\n        b_pred = b_pred.view(-1)\n\n        tp = ((b_gt == 1) & (b_pred == 1)).sum().item()\n        fp = ((b_gt == 0) & (b_pred == 1)).sum().item()\n        fn = ((b_gt == 1) & (b_pred == 0)).sum().item()\n\n        precision = tp / max(tp + fp, 1e-6)\n        recall = tp / max(tp + fn, 1e-6)\n        f1 = 2 * precision * recall / max(precision + recall, 1e-6)\n        return float(f1)\n\n\ndef train_one_epoch(model, dataloader, optimizer, loss_fn, epoch, device, scaler):\n    model.train()\n    running = {\"loss_total\": 0.0, \"loss_main\": 0.0, \"loss_aux\": 0.0, \"loss_boundary\": 0.0}\n\n    pbar = tqdm(dataloader, desc=f\"Train Epoch {epoch}\", leave=False)\n    for imgs, masks in pbar:\n        imgs = imgs.to(device, non_blocking=True)\n        masks = masks.to(device, non_blocking=True)\n\n        optimizer.zero_grad(set_to_none=True)\n        with torch.cuda.amp.autocast(enabled=AMP_ENABLED):\n            outputs = model(imgs)\n            loss, loss_dict = loss_fn(outputs, masks)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        for k in running.keys():\n            if k in loss_dict:\n                running[k] += loss_dict[k]\n\n\n    n = len(dataloader)\n    return {k: v / n for k, v in running.items()}\n\n\ndef eval_one_epoch(model, dataloader, loss_fn, device):\n    model.eval()\n    running = {\"loss_total\": 0.0, \"loss_main\": 0.0, \"loss_aux\": 0.0, \"loss_boundary\": 0.0}\n    cm = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=np.int64)\n    bf1_values = []\n\n    with torch.no_grad():\n        pbar = tqdm(dataloader, desc=\"Validation\", leave=False)\n        for imgs, masks in pbar:\n            imgs = imgs.to(device, non_blocking=True)\n            masks = masks.to(device, non_blocking=True)\n\n            with torch.cuda.amp.autocast(enabled=AMP_ENABLED):\n                outputs = model(imgs)\n                loss, loss_dict = loss_fn(outputs, masks)\n\n            for k in running.keys():\n                if k in loss_dict:\n                    running[k] += loss_dict[k]\n\n\n            logits = outputs[\"logits\"]\n            preds = torch.argmax(logits, dim=1)\n\n            cm_batch = compute_confusion_matrix(preds, masks, NUM_CLASSES, IGNORE_INDEX)\n            cm += cm_batch\n            bf1_values.append(boundary_f1(preds, masks, IGNORE_INDEX))\n\n    n = len(dataloader)\n    loss_stats = {k: v / n for k, v in running.items()}\n    metric_stats, iou_per_class, acc_per_class = metrics_from_confusion_matrix(cm, IGNORE_INDEX)\n    metric_stats[\"BF1\"] = float(np.mean(bf1_values)) if bf1_values else 0.0\n    return loss_stats, metric_stats, iou_per_class, acc_per_class\n\n\ndef enable_mc_dropout(model: nn.Module):\n    for m in model.modules():\n        if isinstance(m, (nn.Dropout, nn.Dropout2d)):\n            m.train()  # keep dropout active in eval\n\n\ndef mc_dropout_predict(model: nn.Module,\n                       imgs: torch.Tensor,\n                       mc_passes: int = 5) -> Tuple[torch.Tensor, torch.Tensor]:\n    model.eval()\n    enable_mc_dropout(model)\n    probs_list = []\n    with torch.no_grad():\n        for _ in range(mc_passes):\n            with torch.cuda.amp.autocast(enabled=AMP_ENABLED):\n                out = model(imgs)\n                logits = out[\"logits\"]\n                probs = F.softmax(logits, dim=1)\n            probs_list.append(probs)\n    probs_stack = torch.stack(probs_list, dim=0)  # [T,B,C,H,W]\n    probs_mean = probs_stack.mean(dim=0)\n    entropy = -(probs_mean * probs_mean.clamp(min=1e-8).log()).sum(dim=1, keepdim=True)\n    return probs_mean, entropy\n\n\n\ndef decode_segmap(mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    mask: [H,W] int labels.\n    returns: [H,W,3] RGB\n    \"\"\"\n    h, w = mask.shape\n    rgb = COLOR_MAP[mask.flatten()].reshape(h, w, 3)\n    return rgb\n\n\ndef save_test_visuals(image_tensor: torch.Tensor,\n                      pred_mask: torch.Tensor,\n                      entropy_map: torch.Tensor,\n                      save_prefix: str):\n  \n    img_np = image_tensor.cpu().numpy()\n    img_np = np.transpose(img_np, (1, 2, 0))\n    img_np = (img_np * np.array([0.229, 0.224, 0.225]) +\n              np.array([0.485, 0.456, 0.406]))\n    img_np = np.clip(img_np, 0, 1)\n\n    mask_np = pred_mask.cpu().numpy()\n    entropy_np = entropy_map.cpu().numpy()\n\n    color_mask = decode_segmap(mask_np)\n    color_mask_norm = color_mask.astype(np.float32) / 255.0\n\n    overlay = 0.6 * img_np + 0.4 * color_mask_norm\n    overlay = np.clip(overlay, 0, 1)\n\n    ent_min, ent_max = entropy_np.min(), entropy_np.max()\n    if ent_max > ent_min:\n        ent_vis = (entropy_np - ent_min) / (ent_max - ent_min)\n    else:\n        ent_vis = np.zeros_like(entropy_np)\n    ent_vis = np.stack([ent_vis, ent_vis, ent_vis], axis=-1)\n\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 3, 1); plt.imshow(img_np); plt.title(\"Image\"); plt.axis(\"off\")\n    plt.subplot(1, 3, 2); plt.imshow(overlay); plt.title(\"Prediction Overlay\"); plt.axis(\"off\")\n    plt.subplot(1, 3, 3); plt.imshow(ent_vis, cmap=\"magma\"); plt.title(\"Uncertainty\"); plt.axis(\"off\")\n    plt.tight_layout()\n    plt.savefig(save_prefix + \"_viz.png\", dpi=150)\n    plt.close()\n\n    mask_rgb = decode_segmap(mask_np)\n    Image.fromarray(mask_rgb).save(save_prefix + \"_mask.png\")\n\n\ndef per_class_dice(cm: np.ndarray, ignore_index: int):\n  \n    tp = np.diag(cm)\n    fp = cm.sum(axis=0) - tp\n    fn = cm.sum(axis=1) - tp\n\n    dice = 2 * tp / np.maximum(2 * tp + fp + fn, 1e-6)\n\n    valid = np.arange(len(dice)) != ignore_index\n    return dice[valid]\n\n\ndef cohens_kappa(cm: np.ndarray):\n\n    total = cm.sum()\n    po = np.trace(cm) / total  # observed accuracy\n\n    # expected accuracy\n    row_marginals = cm.sum(axis=1)\n    col_marginals = cm.sum(axis=0)\n    pe = np.sum(row_marginals * col_marginals) / (total * total)\n\n    kappa = (po - pe) / (1 - pe + 1e-6)\n    return float(kappa)\n\n\ndef boundary_iou(pred: torch.Tensor, target: torch.Tensor, ignore_index: int):\n\n    def extract_boundary(mask: torch.Tensor):\n      \n        if mask.dim() == 2:\n            mask = mask.unsqueeze(0).unsqueeze(0)  # [1,1,H,W]\n        elif mask.dim() == 3:\n            mask = mask.unsqueeze(1)               # [B,1,H,W]\n        else:\n            raise ValueError(\"mask must be 2D or 3D\")\n\n        mask = mask.float()\n\n        max_pool = F.max_pool2d(mask, kernel_size=3, stride=1, padding=1)\n        min_pool = -F.max_pool2d(-mask, kernel_size=3, stride=1, padding=1)\n\n        boundary = (max_pool != min_pool).float()  # [B,1,H,W]\n        return boundary.squeeze(1)                  # [B,H,W]\n\n    with torch.no_grad():\n      \n        t = target.clone()\n        p = pred.clone()\n\n        t[t == ignore_index] = 0\n        p[target == ignore_index] = 0\n\n        bt = extract_boundary(t)   # [1,H,W]\n        bp = extract_boundary(p)   # [1,H,W]\n\n        bt = bt.squeeze(0)\n        bp = bp.squeeze(0)\n\n        intersection = ((bt == 1) & (bp == 1)).sum().item()\n        union = ((bt == 1) | (bp == 1)).sum().item()\n\n        if union == 0:\n            return 0.0\n        \n        return float(intersection / union)\n\n\n\ndef expected_calibration_error(probs: torch.Tensor, targets: torch.Tensor, n_bins=15):\n   \n    with torch.no_grad():\n        conf, pred = probs.max(dim=1)  # [B,H,W]\n\n        pred = pred.view(-1).cpu().numpy()\n        conf = conf.view(-1).cpu().numpy()\n        targets = targets.view(-1).cpu().numpy()\n\n        mask = targets != IGNORE_INDEX\n        pred = pred[mask]\n        conf = conf[mask]\n        targets = targets[mask]\n\n        bins = np.linspace(0.0, 1.0, n_bins + 1)\n        ece = 0.0\n\n        for i in range(n_bins):\n            l, r = bins[i], bins[i+1]\n            idx = (conf >= l) & (conf < r)\n            if np.sum(idx) == 0:\n                continue\n            acc_bin = np.mean(pred[idx] == targets[idx])\n            conf_bin = np.mean(conf[idx])\n            ece += np.abs(acc_bin - conf_bin) * (np.sum(idx) / len(conf))\n\n        return float(ece)\n\n\ndef plot_confusion_matrix(cm, class_names, save_path, normalize=True):\n    \n    if normalize:\n        cm = cm.astype(np.float32)\n        cm = cm / np.maximum(cm.sum(axis=1, keepdims=True), 1e-6)\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt=\".2f\" if normalize else \"d\",\n        cmap=\"Blues\",\n        xticklabels=class_names,\n        yticklabels=class_names\n    )\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Ground Truth\")\n    plt.title(\"Confusion Matrix\" + (\" (Normalized)\" if normalize else \"\"))\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150)\n    plt.close()\n\n\ndef main():\n    print(\"Loading datasets...\")\n\n    train_ds = EarthVQADataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, img_size=IMG_SIZE)\n    val_ds   = EarthVQADataset(VAL_IMG_DIR, VAL_MASK_DIR, img_size=IMG_SIZE)\n    test_ds  = EarthVQATestDataset(TEST_IMG_DIR, img_size=IMG_SIZE)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=8,\n        pin_memory=True,\n        drop_last=True,\n        persistent_workers=True\n    )\n\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=8,\n        pin_memory=True,\n        persistent_workers=True\n    )\n\n    test_loader = DataLoader(\n        test_ds,\n        batch_size=8,\n        shuffle=False,\n        num_workers=8,\n        pin_memory=True\n    )\n\n    print(\"Building model (ConvNeXt-Tiny backbone)...\")\n    model = SegmentationModel(num_classes=NUM_CLASSES).to(DEVICE)\n\n    from thop import profile\n\n    def compute_model_complexity(model, img_size, device):\n        model.eval()\n        dummy = torch.randn(1, 3, img_size, img_size).to(device)\n    \n        flops, params = profile(\n            model,\n            inputs=(dummy,),\n            verbose=False\n        )\n    \n        flops_g = flops / 1e9\n        params_m = params / 1e6\n    \n        return flops_g, params_m\n    gflops, params_m = compute_model_complexity(model, IMG_SIZE, DEVICE)\n\n    print(\"\\n================ MODEL COMPLEXITY ================\")\n    print(f\"Parameters (M): {params_m:.2f}\")\n    print(f\"GFLOPs @ {IMG_SIZE}x{IMG_SIZE}: {gflops:.2f}\")\n    print(\"==================================================\")\n\n\n    with torch.no_grad():\n        x = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)\n        e1, e2, e3, e4 = model.encoder(x)\n        print(\"ENCODER SHAPES:\")\n        print(\"e1:\", e1.shape)\n        print(\"e2:\", e2.shape)\n        print(\"e3:\", e3.shape)\n        print(\"e4:\", e4.shape)\n\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=LR,\n        weight_decay=WEIGHT_DECAY\n    )\n\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer,\n        T_max=NUM_EPOCHS\n    )\n\n    scaler = torch.cuda.amp.GradScaler(enabled=AMP_ENABLED)\n\n    loss_fn = CombinedLoss(\n        num_classes=NUM_CLASSES,\n        ignore_index=IGNORE_INDEX,\n        lambda_main=LAMBDA_MAIN,\n        lambda_aux=LAMBDA_AUX,\n        lambda_boundary=LAMBDA_BOUNDARY\n    )\n\n    start_epoch = 1\n    best_mIoU = 0.0\n    no_improve_epochs = 0\n\n    if RESUME_TRAINING and os.path.exists(BEST_MODEL_PATH):\n        print(f\"\\n[INFO] Resuming training from: {BEST_MODEL_PATH}\")\n\n        ckpt = torch.load(BEST_MODEL_PATH, map_location=DEVICE)\n\n        model.load_state_dict(ckpt[\"model_state\"])\n        optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n        scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n\n        if AMP_ENABLED and ckpt.get(\"scaler_state\") is not None:\n            scaler.load_state_dict(ckpt[\"scaler_state\"])\n\n        start_epoch = ckpt[\"epoch\"] + 1\n        best_mIoU = ckpt[\"best_mIoU\"]\n        no_improve_epochs = ckpt.get(\"no_improve_epochs\", 0)\n\n        print(f\"[INFO] Last epoch finished : {ckpt['epoch']}\")\n        print(f\"[INFO] Resuming from epoch : {start_epoch}\")\n        print(f\"[INFO] Best mIoU so far    : {best_mIoU:.4f}\")\n\n    train_losses = []\n    val_losses = []\n    val_mious = []\n\n    for epoch in range(start_epoch, NUM_EPOCHS + 1):\n        print(f\"\\n=== Epoch {epoch}/{NUM_EPOCHS} ===\")\n\n        train_stats = train_one_epoch(\n            model,\n            train_loader,\n            optimizer,\n            loss_fn,\n            epoch,\n            DEVICE,\n            scaler\n        )\n\n        val_loss_stats, val_metrics, _, _ = eval_one_epoch(\n            model,\n            val_loader,\n            loss_fn,\n            DEVICE\n        )\n\n        scheduler.step()\n\n        train_losses.append(train_stats[\"loss_total\"])\n        val_losses.append(val_loss_stats[\"loss_total\"])\n        val_mious.append(val_metrics[\"mIoU\"])\n\n        print(f\"Train Loss : {train_stats['loss_total']:.4f}\")\n        print(\n            f\"Val Loss   : {val_loss_stats['loss_total']:.4f} | \"\n            f\"mIoU: {val_metrics['mIoU']:.4f} | \"\n            f\"PA: {val_metrics['pixel_acc']:.4f} | \"\n            f\"BF1: {val_metrics['BF1']:.4f}\"\n        )\n\n        mIoU = val_metrics[\"mIoU\"]\n\n        if mIoU > best_mIoU:\n            best_mIoU = mIoU\n            no_improve_epochs = 0\n\n            torch.save({\n                \"epoch\": epoch,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n                \"scheduler_state\": scheduler.state_dict(),\n                \"scaler_state\": scaler.state_dict() if AMP_ENABLED else None,\n                \"best_mIoU\": best_mIoU,\n                \"no_improve_epochs\": no_improve_epochs,\n            }, BEST_MODEL_PATH)\n\n            print(f\"[INFO] New best mIoU: {best_mIoU:.4f} â†’ checkpoint saved\")\n\n        else:\n            no_improve_epochs += 1\n            print(f\"[INFO] No improvement for {no_improve_epochs} epoch(s)\")\n\n        if no_improve_epochs >= EARLY_STOP_PATIENCE:\n            print(\"[INFO] Early stopping triggered.\")\n            break\n\n    print(f\"\\nTraining finished. Best mIoU: {best_mIoU:.4f}\")\n\n    plt.figure(figsize=(8, 5))\n    plt.plot(train_losses, label=\"Train Loss\")\n    plt.plot(val_losses, label=\"Val Loss\")\n    plt.plot(val_mious, label=\"Val mIoU\")\n    plt.xlabel(\"Epoch\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(PLOT_PATH, dpi=150)\n    plt.close()\n\n    print(f\"Training curves saved to {PLOT_PATH}\")\n\n\n    # fINAL TEST INFERENCE\n\n    print(\"\\nLoading best model for final TEST inference...\")\n\n\n  \n    print(\"\\nRunning FINAL METRICS on VALIDATION SET...\")\n    \n    cm_final = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=np.int64)\n    boundary_iou_list = []\n    ece_list = []\n    \n    model.eval()\n    \n    with torch.no_grad():\n        pbar = tqdm(val_loader, desc=\"Final Metrics\")\n        for imgs, masks in pbar:\n            imgs = imgs.to(DEVICE)\n            masks = masks.to(DEVICE)\n    \n            with torch.cuda.amp.autocast(enabled=AMP_ENABLED):\n                out = model(imgs)\n                logits = out[\"logits\"]\n                probs = F.softmax(logits, dim=1)\n                preds = torch.argmax(logits, dim=1)\n    \n            # confusion matrix accumulation\n            cm_final += compute_confusion_matrix(preds, masks, NUM_CLASSES, IGNORE_INDEX)\n    \n            # per-image Boundary IoU\n            for b in range(imgs.size(0)):\n                boundary_iou_list.append(\n                    boundary_iou(preds[b].cpu(), masks[b].cpu(), IGNORE_INDEX)\n                )\n    \n\n            ece_list.append(expected_calibration_error(probs, masks))\n\n    \n\n    tp = np.diag(cm_final)\n    sum_rows = cm_final.sum(axis=1)\n    sum_cols = cm_final.sum(axis=0)\n    union = sum_rows + sum_cols - tp\n    iou_per_class = tp / np.maximum(union, 1e-6)\n    \n\n    dice_valid = per_class_dice(cm_final, IGNORE_INDEX)  # length = 8 (classes 1..8)\n    dice_for_classes = np.full(NUM_CLASSES, np.nan)      # [0..8], start as NaN\n    for cls in range(1, NUM_CLASSES):                    # fill for classes 1..8\n        dice_for_classes[cls] = dice_valid[cls - 1]\n    \n    kappa_final = cohens_kappa(cm_final)\n    boundary_iou_final = float(np.mean(boundary_iou_list)) if boundary_iou_list else 0.0\n    ece_final = float(np.mean(ece_list)) if ece_list else 0.0\n    \n    print(\"\\n================ FINAL METRICS ================\")\n    print(\"Per-class IoU:\", iou_per_class)\n    print(\"Per-class Dice:\", dice_for_classes)\n    print(\"Cohen's Kappa:\", kappa_final)\n    print(\"Boundary IoU:\", boundary_iou_final)\n    print(\"ECE:\", ece_final)\n    print(\"==============================================\")\n\n    CLASS_NAMES = [\n    \"no-data (ignored)\",  # 0\n    \"Background\",         # 1\n    \"Building\",           # 2\n    \"Road\",               # 3\n    \"Water\",              # 4\n    \"Barren\",             # 5\n    \"Forest\",             # 6\n    \"Agriculture\"         # 7\n    ]\n\n    \n    cm_path = os.path.join(WORK_DIR, \"confusion_matrix.png\")\n    plot_confusion_matrix(\n        cm_final,\n        class_names=CLASS_NAMES,\n        save_path=cm_path,\n        normalize=True\n    )\n    \n    print(f\"Confusion matrix saved to: {cm_path}\")\n\n\n    import pandas as pd\n    \n    df = pd.DataFrame({\n        \"class\": list(range(NUM_CLASSES)),\n        \"IoU\": iou_per_class,\n        \"Dice\": dice_for_classes\n    })\n    \n    df2 = pd.DataFrame({\n        \"Metric\": [\"Kappa\", \"BoundaryIoU\", \"ECE\"],\n        \"Value\": [kappa_final, boundary_iou_final, ece_final]\n    })\n    \n    final_csv = os.path.join(WORK_DIR, \"final_metrics.csv\")\n    with open(final_csv, \"w\") as f:\n        df.to_csv(f, index=False)\n        f.write(\"\\n\")\n        df2.to_csv(f, index=False)\n    \n    print(f\"\\nFinal metrics saved to: {final_csv}\")\n\n    ckpt = torch.load(BEST_MODEL_PATH, map_location=DEVICE)\n    model.load_state_dict(ckpt[\"model_state\"])\n    model.to(DEVICE)\n    model.eval()\n\n    print(f\"Running test inference on {len(test_loader.dataset)} images...\")\n    with torch.no_grad():\n        pbar = tqdm(test_loader, desc=\"Test Inference\")\n        for i, (imgs, names) in enumerate(pbar):\n            imgs = imgs.to(DEVICE, non_blocking=True)\n\n     \n            with torch.cuda.amp.autocast(enabled=AMP_ENABLED):\n                out = model(imgs)\n                logits = out[\"logits\"]\n                preds = torch.argmax(logits, dim=1)\n\n       \n            probs_mean, entropy = mc_dropout_predict(model, imgs, mc_passes=5)\n            entropy_map = entropy[0, 0]\n\n            img_name = os.path.splitext(names[0])[0]\n            save_prefix = os.path.join(TEST_OUT_DIR, img_name)\n\n            save_test_visuals(\n                image_tensor=imgs[0].cpu(),\n                pred_mask=preds[0].cpu(),\n                entropy_map=entropy_map.cpu(),\n                save_prefix=save_prefix\n            )\n\n    print(f\"Test predictions + overlays + uncertainty saved in: {TEST_OUT_DIR}\")\n    \n    \n        \n\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}